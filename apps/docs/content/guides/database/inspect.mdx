---
title: 'Debugging and monitoring'
description: 'Inspecting your Postgres database for common issues around disk, query performance, index, locks, and more using the terminal.'
---

Database performance is a large topic and many factors can contribute. Some of the most common causes of poor performance include:

- An inefficiently designed schema
- Inefficiently designed queries
- A lack of indexes causing slower than required queries over large tables
- Unused indexes causing slow `INSERT`, `UPDATE` and `DELETE` operations
- Not enough compute resources, such as memory, causing your database to go to disk for results too often
- Lock contention from multiple queries operating on highly utilized tables
- Large amount of bloat on your tables causing poor query planning

You can examine your database and queries for these issues using either the [Supabase CLI](/docs/guides/local-development/cli/getting-started) or SQL.

## Using the CLI

The Supabase CLI comes with a range of tools to help inspect your Postgres instances for potential issues. The CLI gets the information from <a href="https://www.postgresql.org/docs/current/internals.html" target="_blank">Postgres internals</a>. Therefore, most tools provided are compatible with any Postgres databases regardless if they are a Supabase project or not.

You can find installation instructions for the the Supabase CLI <a href="/docs/guides/cli" target="_blank">here</a>.

### The `inspect db` command

The inspection tools for your Postgres database are under then `inspect db` command. You can get a full list of available commands by running `supabase inspect db help`.

```
$ supabase inspect db help
Tools to inspect your Supabase database

Usage:
  supabase inspect db [command]

Available Commands:
  bloat                Estimates space allocated to a relation that is full of dead tuples
  blocking             Show queries that are holding locks and the queries that are waiting for them to be released
  cache-hit            Show cache hit rates for tables and indices

...
```

### Connect to any Postgres database

Most inspection commands are Postgres agnostic. You can run inspection routines on any Postgres database even if it is not a Supabase project by providing a connection string via `--db-url`.

For example you can connect to your local Postgres instance:

```
supabase --db-url postgresql://postgres:postgres@localhost:5432/postgres inspect db bloat
```

### Connect to a Supabase instance

Working with Supabase, you can link the Supabase CLI with your project:

```
supabase link --project-ref <project-id>
```

Then the CLI will automatically connect to your Supabase project whenever you are in the project folder and you no longer need to provide `â€”db-url`.

### Inspection commands

Below are the `db` inspection commands provided, grouped by different use cases.

<Admonition type="note">

Some commands might require `pg_stat_statements` to be enabled or a specific Postgres version to be used.

</Admonition>

#### Disk storage

These commands are handy if you are running low on disk storage:

- [bloat](/docs/reference/cli/supabase-inspect-db-bloat) - estimates the amount of wasted space
- [vacuum-stats](/docs/reference/cli/supabase-inspect-db-vacuum-stats) - gives information on waste collection routines
- [table-record-counts](/docs/reference/cli/supabase-inspect-db-table-record-counts) - estimates the number of records per table
- [table-sizes](/docs/reference/cli/supabase-inspect-db-table-sizes) - shows the sizes of tables
- [index-sizes](/docs/reference/cli/supabase-inspect-db-index-sizes) - shows the sizes of individual index
- [table-index-sizes](/docs/reference/cli/supabase-inspect-db-table-index-sizes) - shows the sizes of indexes for each table

#### Query performance

The commands below are useful if your Postgres database consumes a lot of resources like CPU, RAM or Disk IO. You can also use them to investigate slow queries.

- [cache-hit](/docs/reference/cli/supabase-inspect-db-cache-hit) - shows how efficient your cache usage is overall
- [unused-indexes](/docs/reference/cli/supabase-inspect-db-unused-indexes) - shows indexes with low index scans
- [index-usage](/docs/reference/cli/supabase-inspect-db-index-usage) - shows information about the efficiency of indexes
- [seq-scans](/docs/reference/cli/supabase-inspect-db-seq-scans) - show number of sequential scans recorded against all tables
- [long-running-queries](/docs/reference/cli/supabase-inspect-db-long-running-queries) - shows long running queries that are executing right now
- [outliers](/docs/reference/cli/supabase-inspect-db-outliers) - shows queries with high execution time but low call count and queries with high proportion of execution time spent on synchronous I/O

#### Locks

- [locks](/docs/reference/cli/supabase-inspect-db-locks) - shows statements which have taken out an exclusive lock on a relation
- [blocking](/docs/reference/cli/supabase-inspect-db-blocking) - shows statements that are waiting for locks to be released

#### Connections

- [role-connections](/docs/reference/cli/supabase-inspect-db-role-connections) - shows number of active connections for all database roles (Supabase-specific command)
- [replication-slots](/docs/reference/cli/supabase-inspect-db-replication-slots) - shows information about replication slots on the database

### Notes on `pg_stat_statements`

Following commands require `pg_stat_statements` to be enabled: calls, locks, cache-hit, blocking, unused-indexes, index-usage, bloat, outliers, table-record-counts, replication-slots, seq-scans, vacuum-stats, long-running-queries.

When using `pg_stat_statements` also take note that it only stores the latest 5,000 statements. Moreover, consider resetting the analysis after optimizing any queries by running `select pg_stat_statements_reset();`

Learn more about pg_stats [here](https://supabase.com/docs/guides/database/extensions/pg_stat_statements).

## Using SQL

<Admonition type="note">

If you're seeing an `insufficient privilege` error when viewing the Query Performance page from the dashboard, run this command:

```shell
$ grant pg_read_all_stats to postgres;
```

</Admonition>

### Postgres cumulative statistics system

Postgres collects data about its own operations using the [cumulative statistics system](https://www.postgresql.org/docs/current/monitoring-stats.html). In addition to this, every Supabase project has the [pg_stat_statements extension](/docs/guides/database/extensions/pg_stat_statements) enabled by default. This extension records query execution performance details and is the best way to find inefficient queries. This information can be combined with the Postgres query plan analyzer to develop more efficient queries.

Here are some example queries to get you started.

### Most frequently called queries

```sql
select
  auth.rolname,
  statements.query,
  statements.calls,
  -- -- Postgres 13, 14, 15
  statements.total_exec_time + statements.total_plan_time as total_time,
  statements.min_exec_time + statements.min_plan_time as min_time,
  statements.max_exec_time + statements.max_plan_time as max_time,
  statements.mean_exec_time + statements.mean_plan_time as mean_time,
  -- -- Postgres <= 12
  -- total_time,
  -- min_time,
  -- max_time,
  -- mean_time,
  statements.rows / statements.calls as avg_rows
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by statements.calls desc
limit 100;
```

This query shows:

- query statistics, ordered by the number of times each query has been executed
- the role that ran the query
- the number of times it has been called
- the average number of rows returned
- the cumulative total time the query has spent running
- the min, max and mean query times.

This provides useful information about the queries you run most frequently. Queries that have high `max_time` or `mean_time` times and are being called often can be good candidates for optimization.

### Slowest queries by execution time

```sql
select
  auth.rolname,
  statements.query,
  statements.calls,
  -- -- Postgres 13, 14, 15
  statements.total_exec_time + statements.total_plan_time as total_time,
  statements.min_exec_time + statements.min_plan_time as min_time,
  statements.max_exec_time + statements.max_plan_time as max_time,
  statements.mean_exec_time + statements.mean_plan_time as mean_time,
  -- -- Postgres <= 12
  -- total_time,
  -- min_time,
  -- max_time,
  -- mean_time,
  statements.rows / statements.calls as avg_rows
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by max_time desc
limit 100;
```

This query will show you statistics about queries ordered by the maximum execution time. It is similar to the query above ordered by calls, but this one highlights outliers that may have high executions times. Queries which have high or mean execution times are good candidates for optimization.

### Most time consuming queries

```sql
select
  auth.rolname,
  statements.query,
  statements.calls,
  statements.total_exec_time + statements.total_plan_time as total_time,
  to_char(
    (
      (statements.total_exec_time + statements.total_plan_time) / sum(
        statements.total_exec_time + statements.total_plan_time
      ) over ()
    ) * 100,
    'FM90D0'
  ) || '%' as prop_total_time
from
  pg_stat_statements as statements
  inner join pg_authid as auth on statements.userid = auth.oid
order by total_time desc
limit 100;
```

This query will show you statistics about queries ordered by the cumulative total execution time. It shows the total time the query has spent running as well as the proportion of total execution time the query has taken up.

Queries which are the most time consuming are not necessarily bad, you may have a very efficient and frequently ran queries that end up taking a large total % time, but it can be useful to help spot queries that are taking up more time than they should.

### Hit rate

Generally for most applications a small percentage of data is accessed more regularly than the rest. To make sure that your regularly accessed data is available, Postgres tracks your data access patterns and keeps this in its [shared_buffers](https://www.postgresql.org/docs/15/runtime-config-resource.html#RUNTIME-CONFIG-RESOURCE-MEMORY) cache.

Applications with lower cache hit rates generally perform more poorly since they have to hit the disk to get results rather than serving them from memory. Very poor hit rates can also cause you to burst past your [Disk IO limits](./compute-add-ons#disk-io) causing significant performance issues.

You can view your cache and index hit rate by executing the following query:

```sql
select
  'index hit rate' as name,
  (sum(idx_blks_hit)) / nullif(sum(idx_blks_hit + idx_blks_read), 0) * 100 as ratio
from pg_statio_user_indexes
union all
select
  'table hit rate' as name,
  sum(heap_blks_hit) / nullif(sum(heap_blks_hit) + sum(heap_blks_read), 0) * 100 as ratio
from pg_statio_user_tables;
```

This shows the ratio of data blocks fetched from the Postgres [shared_buffers](https://www.postgresql.org/docs/15/runtime-config-resource.html#RUNTIME-CONFIG-RESOURCE-MEMORY) cache against the data blocks that were read from disk/OS cache.

If either of your index or table hit rate are < 99% then this can indicate your compute plan is too small for your current workload and you would benefit from more memory. [Upgrading your compute](./compute-add-ons) is easy and can be done from your [project dashboard](https://supabase.com/dashboard/project/_/settings/compute-and-disk).

### Optimizing poor performing queries

Postgres has built in tooling to help you optimize poorly performing queries. You can use the [query plan analyzer](https://www.postgresql.org/docs/current/sql-explain.html) on any expensive queries that you have identified:

```sql
explain analyze <query-statement-here>;
```

When you include `analyze` in the explain statement, the database attempts to execute the query and provides a detailed query plan along with actual execution times. So, be careful using `explain analyze` with `insert`/`update`/`delete` queries, because the query will actually run, and could have unintended side-effects.

If you run just `explain` without the `analyze` keyword, the database will only perform query planning without actually executing the query. This approach can be beneficial when you want to inspect the query plan without affecting the database or if you encounter timeouts in your queries.

Using the query plan analyzer to optimize your queries is a large topic, with a number of online resources available:

- [Official docs.](https://www.postgresql.org/docs/current/using-explain.html)
  {/* supa-mdx-lint-disable-next-line Rule004ExcludeWords */}
- [The Art of PostgreSQL.](https://theartofpostgresql.com/explain-plan-visualizer/)
- [Postgres Wiki.](https://wiki.postgresql.org/wiki/Using_EXPLAIN)
- [Enterprise DB.](https://www.enterprisedb.com/blog/postgresql-query-optimization-performance-tuning-with-explain-analyze)

You can pair the information available from `pg_stat_statements` with the detailed system metrics available [via your metrics endpoint](../platform/metrics) to better understand the behavior of your DB and the queries you're executing against it.

## Using the Dashboard

The Supabase Dashboard provides historical monitoring charts that visualize key database metrics. These charts help you identify performance bottlenecks, resource constraints, and optimization opportunities at a glance.

<Admonition type="note">

Advanced Database monitoring charts are currently available only for Teams and Enterprise plans.
[Upgrade your plan](https://supabase.com/dashboard/org/_/billing) to access these features.

</Admonition>

### Memory usage

<Image
  alt="Memory usage chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/memory-usage-chart-dark.png',
    light: '/docs/img/database/reports/memory-usage-chart-light.png',
  }}
/>

This chart shows memory allocation across:

- **Used**: RAM actively used by Postgres and the operating system
- **Cache + buffers**: Memory used for page cache and PostgreSQL buffers
- **Free**: Available unallocated memory

**How it helps debug issues:**

- Identify memory pressure when free memory is consistently low
- Monitor cache effectiveness for query performance
- Detect memory leaks or inefficient memory usage

**Actions you can take:**

- [Upgrade compute size](https://supabase.com/docs/guides/platform/compute-and-disk#compute-size) for more memory
- Optimize queries that consume excessive memory
- Tune PostgreSQL configuration for better memory management
- Implement query result caching at the application level

### CPU usage

<Image
  alt="CPU usage chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/cpu-usage-chart-dark.png',
    light: '/docs/img/database/reports/cpu-usage-chart-light.png',
  }}
/>

This chart breaks down CPU usage by category:

- **System**: CPU time for kernel operations
- **User**: CPU time for database queries and user processes
- **IOwait**: CPU time waiting for disk/network IO
- **IRQs**: CPU time handling interrupts
- **Other**: CPU time for miscellaneous tasks

**How it helps debug issues:**

- Identify CPU-intensive queries when User CPU is high
- Detect IO bottlenecks when IOwait is elevated
- Monitor system overhead and resource contention

**Actions you can take:**

- Optimize CPU-intensive queries identified through high User CPU
- Address IO bottlenecks if IOwait is consistently high
- [Upgrade compute size](/docs/guides/platform/compute-and-disk) for more CPU capacity
- Use query optimization techniques and [proper indexing](/docs/guides/database/postgres/indexes)

### Disk Input/Output operations per second (IOPS)

<Image
  alt="Disk IOPS chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/disk-iops-chart-dark.png',
    light: '/docs/img/database/reports/disk-iops-chart-light.png',
  }}
/>

This chart displays read and write IOPS with a reference line showing your compute size's maximum IOPS capacity.

**How it helps debug issues:**

- Identify when disk IO becomes a bottleneck
- Distinguish between read-heavy vs write-heavy workloads
- Spot spikes in disk activity that correlate with performance issues

**Actions you can take:**

- Optimize queries causing high read IOPS through better indexing
- Consider read replicas for read-heavy workloads
- Batch write operations to reduce write IOPS
- [Upgrade compute size](https://supabase.com/docs/guides/platform/compute-and-disk) for higher IOPS limits

### Disk IO Usage

<Image
  alt="Disk IO Usage chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/disk-io-chart-dark.png',
    light: '/docs/img/database/reports/disk-io-chart-light.png',
  }}
/>

This chart displays the percentage of your allocated IOPS (Input/Output Operations Per Second) currently being used.

**How it helps debug issues:**

- Identify when you're approaching your IOPS limits
- Correlate high IO usage with performance issues
- Monitor the impact of database operations on disk performance

**Actions you can take:**

- Optimize queries that perform excessive disk reads/writes
- Add appropriate indexes to reduce sequential scans
- Consider upgrading to a [larger compute size](https://supabase.com/docs/guides/platform/compute-and-disk) for higher IOPS limits
- Review and optimize your database schema and query patterns

### Disk Size

<Image
  alt="Disk Size chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/disk-size-chart-dark.png',
    light: '/docs/img/database/reports/disk-size-chart-light.png',
  }}
/>

This chart shows disk usage breakdown including:

- **Database**: Space used by your actual database data (tables, indexes)
- **WAL**: Space used by Write-Ahead Logging
- **System**: Reserved space for system operations

**How it helps debug issues:**

- Monitor disk space consumption trends
- Identify rapid growth that may require attention
- Plan for disk space upgrades before hitting limits

**Actions you can take:**

- Run [VACUUM](https://www.postgresql.org/docs/current/sql-vacuum.html) operations to reclaim dead tuple space
- Identify and optimize large tables using the CLI commands like `table-sizes`
- Consider data archival strategies for historical data
- [Upgrade disk size](https://supabase.com/docs/guides/platform/database-size) when approaching capacity

### Database client connections

<Image
  alt="Database client connections chart"
  zoomable
  src={{
    dark: '/docs/img/database/reports/db-client-connections-chart-dark.png',
    light: '/docs/img/database/reports/db-client-connections-chart-light.png',
  }}
/>

This chart shows the number of active connections to your database broken down by connection type:

- **Postgres**: Direct connections from your application
- **PostgREST**: Connections from the PostgREST API layer
- **Reserved**: Administrative connections for Supabase services
- **Auth**: Connections from Supabase Auth service
- **Storage**: Connections from Supabase Storage service
- **Other roles**: Miscellaneous database connections

**How it helps debug issues:**

- Identify connection pool exhaustion when approaching max connections
- Spot connection leaks from applications not properly closing connections
- Monitor connection distribution across different services

**Actions you can take:**

- [Upgrade your compute size](https://supabase.com/docs/guides/platform/compute-and-disk#compute-size) to increase connection limits
- Implement [connection pooling](https://supabase.com/docs/guides/database/connecting-to-postgres#shared-pooler) for better connection management in your application if you see high direct connections
- Review your application code for proper connection handling
